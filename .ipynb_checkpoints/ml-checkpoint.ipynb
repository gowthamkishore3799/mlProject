{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0b250d-9d52-46dc-bd6a-221632ed538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gowthamkishorevijay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             subject  \\\n",
      "0                          Never agree to be a loser   \n",
      "1                             Befriend Jenna Jameson   \n",
      "2                                  CNNcom Daily Top    \n",
      "3  Re svn commit r  in spamassassintrunk libMailS...   \n",
      "4                         SpecialPricesPharmMoreinfo   \n",
      "\n",
      "                                                body  \n",
      "0  Buck up your troubles caused by small dimensio...  \n",
      "1  \\nUpgrade your sex and pleasures with these te...  \n",
      "2   THE DAILY TOP  from CNNcom Top videos and sto...  \n",
      "3  Would anyone object to removing so from this l...  \n",
      "4  \\nWelcomeFastShippingCustomerSupport\\nhttpiwfn...  \n",
      "                                              sender  \\\n",
      "0                   Young Esposito <Young@iworld.de>   \n",
      "1                       Mok <ipline's1983@icable.ph>   \n",
      "2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
      "3                 Michael Parker <ivqrnai@pobox.com>   \n",
      "4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
      "\n",
      "                                         receiver  \\\n",
      "0                     user4@gvc.ceas-challenge.cc   \n",
      "1                   user2.2@gvc.ceas-challenge.cc   \n",
      "2                   user2.9@gvc.ceas-challenge.cc   \n",
      "3  SpamAssassin Dev <xrh@spamassassin.apache.org>   \n",
      "4                   user2.2@gvc.ceas-challenge.cc   \n",
      "\n",
      "                              date  \\\n",
      "0  Tue, 05 Aug 2008 16:31:02 -0700   \n",
      "1  Tue, 05 Aug 2008 18:31:03 -0500   \n",
      "2  Tue, 05 Aug 2008 20:28:00 -1200   \n",
      "3  Tue, 05 Aug 2008 17:31:20 -0600   \n",
      "4  Tue, 05 Aug 2008 19:31:21 -0400   \n",
      "\n",
      "                                             subject  \\\n",
      "0                          Never agree to be a loser   \n",
      "1                             Befriend Jenna Jameson   \n",
      "2                                  CNNcom Daily Top    \n",
      "3  Re svn commit r  in spamassassintrunk libMailS...   \n",
      "4                         SpecialPricesPharmMoreinfo   \n",
      "\n",
      "                                                body  label  urls  \n",
      "0  Buck up your troubles caused by small dimensio...      1     1  \n",
      "1  \\nUpgrade your sex and pleasures with these te...      1     1  \n",
      "2   THE DAILY TOP  from CNNcom Top videos and sto...      1     1  \n",
      "3  Would anyone object to removing so from this l...      0     1  \n",
      "4  \\nWelcomeFastShippingCustomerSupport\\nhttpiwfn...      1     1  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Set the NLTK data download path\n",
    "# nltk_data_path = '/Users/gowthamkishorevijay/Desktop/Playground/projects/my-venv/CEAS_08.csv'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "# if not os.path.exists(nltk_data_path):\n",
    "#     os.makedirs(nltk_data_path)\n",
    "\n",
    "# # Add the path where NLTK data will be downloaded\n",
    "# nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# # Ensure necessary NLTK resources are downloaded\n",
    "# nltk.download('punkt', download_dir=nltk_data_path)\n",
    "# nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "# nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "# nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load the CSV file (replace this path with your actual file path)\n",
    "file_path = '/Users/gowthamkishorevijay/Desktop/Playground/projects/my-venv/CEAS_08.csv'  # Adjust based on your local environment\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Check if the value is a string, otherwise convert it to an empty string\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function\n",
    "if 'subject' in df.columns and 'body' in df.columns:\n",
    "    # Fill NaN values with empty strings\n",
    "    df['subject'] = df['subject'].fillna('')\n",
    "    df['body'] = df['body'].fillna('')\n",
    "    \n",
    "    # Apply the clean_text function to 'subject' and 'body'\n",
    "    df['subject'] = df['subject'].apply(clean_text)\n",
    "    df['body'] = df['body'].apply(clean_text)\n",
    "\n",
    "    # Display cleaned data\n",
    "    print(df[['subject', 'body']].head())\n",
    "else:\n",
    "    print(\"The CSV file does not contain 'subject' or 'body' columns.\")\n",
    "df.to_csv('preprocessed_emails.csv', index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040e9273-8bd4-48a3-87ac-aae9598a8116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10951    1\n",
      "8668     1\n",
      "8200     1\n",
      "14981    0\n",
      "23343    0\n",
      "        ..\n",
      "6265     0\n",
      "11284    1\n",
      "38158    0\n",
      "860      1\n",
      "15795    0\n",
      "Name: label, Length: 31323, dtype: int64\n",
      "\n",
      "Train ROC AUC: 1.0\n",
      "Test ROC AUC: 0.9889738608813544\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m fpr_train, tpr_train, thresholds_train \u001b[38;5;241m=\u001b[39m roc_curve(y_train, pred_prob_train)\n\u001b[1;32m     29\u001b[0m fpr_test, tpr_test, thresholds_test \u001b[38;5;241m=\u001b[39m roc_curve(y_test, pred_prob_test)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(fpr_train, tpr_train, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain ROC AUC: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(roc_auc_train))\n\u001b[1;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(fpr_test, tpr_test, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest ROC AUC: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(roc_auc_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "X = df['body'] \n",
    "y = df['label'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),  # Step 1: Text data transformation\n",
    "    ('nb', RandomForestClassifier())  # Step 2: Classification using Naive Bayes\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "pred_prob_train = model.predict_proba(X_train)[:,1]\n",
    "pred_prob_test = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "# calculate ROC AUC score\n",
    "roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(\"\\nTrain ROC AUC:\", roc_auc_train)\n",
    "print(\"Test ROC AUC:\", roc_auc_test)\n",
    "\n",
    "# plot the ROC curve\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, pred_prob_train)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, pred_prob_test)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr_train, tpr_train, label=\"Train ROC AUC: {:.2f}\".format(roc_auc_train))\n",
    "plt.plot(fpr_test, tpr_test, label=\"Test ROC AUC: {:.2f}\".format(roc_auc_test))\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()\n",
    "\n",
    "# calculate confusion matrix\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(11,4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "sns.heatmap(cm_train, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[0])\n",
    "ax[0].set_xlabel(\"Predicted Label\")\n",
    "ax[0].set_ylabel(\"True Label\")\n",
    "ax[0].set_title(\"Train Confusion Matrix\")\n",
    "\n",
    "sns.heatmap(cm_test, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[1])\n",
    "ax[1].set_xlabel(\"Predicted Label\")\n",
    "ax[1].set_ylabel(\"True Label\")\n",
    "ax[1].set_title(\"Test Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd97b439-1d00-4394-b2c4-23d281734053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Spam Email!\n"
     ]
    }
   ],
   "source": [
    "# new_email = \"\"\"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\"\"\"\n",
    "new_email = \"How are you mate\"\n",
    "\n",
    "\n",
    "prediction = model.predict([new_email])\n",
    "\n",
    "if prediction == 0:\n",
    "    print( \"This is a not a spam Email!\")\n",
    "else:\n",
    "    print( \"This is a Spam Email!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06448f76-2357-4583-9dc4-e7fe36f70569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
